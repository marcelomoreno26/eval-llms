import datasets
import evaluate
import re
import numpy as np
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams



_CITATION = """
@misc{kim2024prometheus2opensource,
      title={Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models}, 
      author={Seungone Kim and Juyoung Suk and Shayne Longpre and Bill Yuchen Lin and Jamin Shin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2024},
      eprint={2405.01535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.01535}, 
}
"""



_DESCRIPTION = """
The Prometheus Score is generated by one of the Prometheus Feedback collection models described in the
Prometheus 2 paper using the direct assesment task. In direct assessment, the model assigns a score to each response based on predefined criteria. 
In the case of this metric the Prometheus evaluates models responses based on five criteria: Veracity, Relevance, 
Completeness, Fidelity to Documentation, and Clarity and Coherence. Each response is scored from 1 to 10, 
reflecting its alignment with these criteria. The average score for all responses is calculated.
"""



_KWARGS_DESCRIPTION = """
Compute the Prometheus Score by evaluating model responses based on predefined criteria.

Args:
    predictions: List of predictions (responses generated by the language model).
    references: List of references (expected responses or ground truth).
    contexts: List of context information or documentation that supports the conversation.
    model_name: String specifying the name of the language model to be used for evaluation.
    previous_conversations (optional): List of previous conversations in the form of a list of dictionaries, where each dictionary contains "role" and "content" keys, reflecting the user's and model's dialogues.
    return_feedbacks (optional): Boolean flag. If set to `True`, the function will return feedbacks along with the scores.
    return_average (optional): Boolean flag. If set to `True`, the function will return the average score across all predictions.

Returns:
    If `return_feedbacks` and `return_average` are both `True`, returns a tuple containing:
        - A list of integer scores (one score for each prediction).
        - A list of feedback strings corresponding to each prediction.
        - A float representing the average score across all predictions.
    If only `return_feedbacks` is `True`, returns a tuple containing:
        - A list of integer scores (one score for each prediction).
        - A list of feedback strings corresponding to each prediction.
    If only `return_average` is `True`, returns a tuple containing:
        - A list of integer scores (one score for each prediction).
        - A float representing the average score across all predictions.
    Otherwise, returns a list of integer scores (one score for each prediction).

Examples:

    >>> predictions = [
    ...     "El sol brilla debido a reacciones de fusión nuclear en su núcleo.",
    ...     "La capital de Francia es París."
    ... ]
    >>> references = [
    ...     "El sol produce luz y calor mediante reacciones de fusión nuclear en su núcleo.",
    ...     "París es la capital de Francia."
    ... ]
    >>> contexts = [
    ...     "El Sol emite energía en forma de luz y calor, generada a través de reacciones de fusión nuclear en su núcleo.",
    ...     "Francia es un país europeo cuya capital es París."
    ... ]
    >>> metric = PrometheusScore()
    >>> scores, feedbacks, avg_score = metric.compute(
    ...     predictions=predictions,
    ...     references=references,
    ...     contexts=contexts,
    ...     model_name="model_name_here",
    ...     return_feedbacks=True,
    ...     return_average=True
    ... )
    >>> print(scores)
        [9, 10]
    >>> print(feedbacks)
        ["Respuesta precisa y clara. Puntuación: 9", "Respuesta correcta. Puntuación: 10"]
    >>> print(avg_score)
        9.5
"""



template = """### Descripción de la Tarea:

A continuación, se te proporcionará una conversación entre un usuario y un modelo de lenguaje. Tu tarea es evaluar la última respuesta del modelo de lenguaje en base a los siguientes criterios:

1. Veracidad: La respuesta debe ser honesta, sincera y verídica según la información disponible.
2. Relevancia: La respuesta debe estar directamente relacionada con la pregunta o solicitud del usuario.
3. Completitud: La respuesta debe ser lo más completa posible para ser de ayuda al usuario, proporcionando toda la información relevante disponible.
4. Fidelidad a la Documentación: La respuesta debe ser fiel a la documentación proporcionada, sin inventar información.
5. Claridad y Coherencia: La respuesta debe ser clara, coherente y fácil de entender para el usuario. Debe utilizar un español correcto y comprensible.

Por favor, proporciona una puntuación del 1 al 10 para la respuesta dada por el modelo teniendo en cuenta estos 5 criterios y un feedback detallado. En caso de que no haya una respuesta disponible del modelo, devuelve únicamente "Puntuación: 0".

Instrucciones:

1. Lee el "input" proporcionado por el usuario.
2. Lee la conversación que han mantenido previamente el usuario y el modelo de lenguaje.
3. Lee la respuesta última del modelo de lenguaje.
4. Proporciona un feedback detallado que explique las razones detrás de tu puntuación. No utilices más de 100 palabras para este feedback, ha de ser conciso y breve.
5. Evalúa de 1 a 10 la respuesta del modelo de lenguaje basándote en los criterios mencionados, siguiendo el feedback del apartado anterior. Es muy importante que únicamente generes una puntuación en formato numérico de 1 a 10 en este apartado.

---

### Documentación de apoyo a la conversación (contextos informativos)

{context}

###  Conversación previa

{previous_conversation}

### Respuesta del Modelo

{prediction}

### Respuesta de Referencia:

{reference}

---
* Formato de la Respuesta *
### Feedback
[Tu feedback]
Puntuación: [1-10]
---

### Feedback
"""



@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
class PrometheusScore(evaluate.Metric):
    def _info(self):
        return evaluate.MetricInfo(
            description=_DESCRIPTION,
            citation=_CITATION,
            inputs_description=_KWARGS_DESCRIPTION,
            features=datasets.Features(
                {
                    "predictions": datasets.Value(dtype="string"),
                    "references": datasets.Value(dtype="string"),
                },
            ),
            reference_urls=["https://arxiv.org/abs/2405.01535"],
        )
    

    def format_conversation(self, conv: list[dict]):
        role_map = {"assistant": "asistente", "user": "usuario"}
        return "\n".join([f"{role_map[mssg['role']]}: {mssg['content']}" for mssg in conv])


    def extract_score(self, feedback: str):
        pattern = r'Puntuación: (\d{1,2})|Puntuación: (\d{1,2}/10)|\(\d{1,2}/10\)'
        matches = re.search(pattern, feedback)
        if matches:
            if matches.group(1):
                return int(matches.group(1))
            elif matches.group(2):
                return int(matches.group(2))
            elif matches.group(0):
                return int(matches.group(0).strip('()').split('/')[0])
        return np.nan
    

    def _compute(
        self,
        model_name: str,
        predictions: list[str],
        references: list[str],
        contexts: list[str],
        previous_conversations: list[list[dict]] = [],
        return_feedbacks: bool = False,
        return_average: bool = False
    ) -> list[int] | tuple[list[int], list[str]] | tuple[list[int], float] | tuple[list[int], list[str], float]:

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = LLM(model_name)
        messages = []

        if previous_conversations:
            for prediction, reference, context, previous_conversation in zip(predictions, references, contexts, previous_conversations):
                prompt = template.format(
                    context = context,
                    previous_conversation = self.format_conversation(previous_conversation),
                    prediction = prediction,
                    reference = reference
                )

                message = tokenizer.apply_chat_template([{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True)
                messages.append(message)
        else:
            for prediction, reference, context in zip(predictions, references, contexts):
                prompt = template.format(
                    context = context,
                    previous_conversation = "No hay conversación previa.",
                    prediction = prediction,
                    reference = reference
                )

                message = tokenizer.apply_chat_template([{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True)
                messages.append(message)
        
        sampling_params = SamplingParams(max_tokens=1024, top_p=0.8, temperature=0.3, repetition_penalty=1.05, min_p= 0.1, stop=["<|eot_id|>", "<|im_end|>"])
        outputs =  model.generate(messages, sampling_params=sampling_params)
        feedbacks = [output.outputs[0].text for output in outputs]
        scores = list(map(self.extract_score, feedbacks))


        if return_average:
            avg_score = float(np.nanmean(scores))
            if return_feedbacks:
                return scores, feedbacks, avg_score
            else:
                return scores, avg_score
        elif return_feedbacks:
            return scores, feedbacks

        return scores